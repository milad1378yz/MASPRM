from dataclasses import dataclass, field
from typing import Optional, Callable, Dict, List, Tuple, Any
import os
import random
import math
from pathlib import Path

import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
from peft import PeftModel, PeftConfig

from answer_utils import numbers_equal
from mas import MAS
from mcts import propose_agent_candidates, _replay_trajectory, _aggregate_final
from agent import Agent


def _seed_everything(seed: int) -> None:
    try:
        os.environ["PYTHONHASHSEED"] = str(int(seed))
    except Exception:
        pass
    random.seed(int(seed))
    try:
        np.random.seed(int(seed))
    except Exception:
        pass
    torch.manual_seed(int(seed))


@dataclass
class TokenStats:
    prompt: int = 0  # tokens in prompts sent to the policy LM
    generated: int = 0  # tokens generated by the policy LM
    scorer: int = 0  # tokens processed by scoring models (PRM/logprob/ORM)
    prm_calls: int = 0  # how many times PRM was called
    agent_runs: int = 0  # how many times we ran a single agent (one step decode)

    def total(self) -> int:
        return self.prompt + self.generated + self.scorer

    def add(self, other: "TokenStats") -> "TokenStats":
        self.prompt += other.prompt
        self.generated += other.generated
        self.scorer += other.scorer
        self.prm_calls += other.prm_calls
        self.agent_runs += other.agent_runs
        return self


# ======================
#  Token length helpers
# ======================


def _text_token_len_tok(tok, text: str) -> int:
    return int(tok(text, return_tensors="pt", add_special_tokens=False)["input_ids"].shape[1])


def _text_token_len_agent(agent, text: str) -> int:
    return _text_token_len_tok(agent.tok, text)


# ===========================
#  LOGPROB scoring utilities
# ===========================


@torch.inference_mode()
def compute_logprob_and_token_counts(
    agent: Agent,
    msgs=None,
    completion: str = "",
    *,
    prompt_ids=None,  # pass pre-encoded prompt to avoid re-tokenization
) -> tuple[float, int, int]:
    """
    Returns (sum_logprob, prompt_len, gen_len).
    Vectorized; avoids Python loops. If `prompt_ids` is provided (list[int] or Tensor),
    `msgs` is ignored.
    """
    tok = agent.tok
    model = agent.model
    device = next(model.parameters()).device

    # Resolve prompt_ids (B=1)
    if prompt_ids is None:
        enc = tok.apply_chat_template(msgs, tokenize=True, add_generation_prompt=True)
        if isinstance(enc, list):
            prompt_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)
        elif isinstance(enc, torch.Tensor):
            prompt_ids = (enc.unsqueeze(0) if enc.dim() == 1 else enc).to(device)
        else:
            s = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            prompt_ids = tok(s, return_tensors="pt", add_special_tokens=False)["input_ids"].to(
                device
            )
    else:
        if isinstance(prompt_ids, list):
            prompt_ids = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0).to(device)
        elif isinstance(prompt_ids, torch.Tensor):
            prompt_ids = (prompt_ids.unsqueeze(0) if prompt_ids.dim() == 1 else prompt_ids).to(
                device
            )
        else:
            raise TypeError("prompt_ids must be list[int] or torch.Tensor")

    comp_ids = tok(completion, return_tensors="pt", add_special_tokens=False)["input_ids"].to(
        device
    )
    input_ids = torch.cat([prompt_ids, comp_ids], dim=1)  # [1, L]
    attn = torch.ones_like(input_ids, device=device)

    out = model(input_ids=input_ids, attention_mask=attn)
    logits = out.logits[:, :-1, :]  # next-token logits, [1, L-1, V]
    target = input_ids[:, 1:]  # tokens to predict, [1, L-1]

    prompt_len = int(prompt_ids.shape[1])
    gen_len = int(comp_ids.shape[1])
    start = max(0, prompt_len - 1)  # first position predicting gen token 0
    end = start + gen_len

    # Vectorized gather of logprobs for generated tokens
    lp_slice = F.log_softmax(logits[0, start:end, :], dim=-1)  # [g, V]
    tgt_slice = target[0, start:end].unsqueeze(-1)  # [g, 1]
    s = lp_slice.gather(-1, tgt_slice).sum().item()  # scalar

    return float(s), prompt_len, gen_len


def score_path_logprob(
    mas,
    question: str,
    steps: list[str],
    *,
    agg: str = "sum",
) -> tuple[float, TokenStats]:
    required_parents = {i: set(mas.parents.get(i, [])) for i in range(mas.n)}
    usage = TokenStats()
    traj_so_far = {}
    s_total = 0.0
    token_total = 0

    for j, text in enumerate(steps):
        inbox, _, _ = _replay_trajectory(mas, question, traj_so_far)
        have = inbox.get(j, {})
        ordered_parents = sorted(required_parents[j])
        inputs = [have[p] for p in ordered_parents if p in have]
        sys_prompt = getattr(mas.agents[j], "system_prompt", "You are a helpful assistant.")
        msgs = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": "\n\n".join(inputs).strip()},
        ]

        lp, p_len, g_len = compute_logprob_and_token_counts(mas.agents[j], msgs, text)
        usage.scorer += p_len + g_len
        s_total += lp
        token_total += max(0, g_len)

        traj_so_far[j] = [text]

    if agg in ("avg_token", "avg", "mean_token"):
        value = s_total / max(1, token_total)
    elif agg in ("avg_step", "mean_step"):
        value = s_total / max(1, len(steps))
    else:  # "sum" (default)
        value = s_total

    return value, usage


def score_path_prm_product(
    score_fn, mas, question: str, steps: list[str], tokenizer=None, step_separator: str = "</step>"
) -> tuple[float, TokenStats]:
    usage = TokenStats()
    traj = {}
    product = 1.0
    for j, text in enumerate(steps):
        traj[j] = [text]
        s_text = render_state_text(mas, question, traj, step_separator)
        v = float(score_fn(s_text))  # [-1, 1]
        usage.prm_calls += 1
        w = (v + 1.0) / 2.0
        product *= max(1e-6, w)
        if tokenizer is not None:
            usage.scorer += _text_token_len_tok(tokenizer, s_text)
    return product, usage


def score_orm_end(orm_fn, answer_text: str, tokenizer=None) -> tuple[float, TokenStats]:
    usage = TokenStats()
    v = float(orm_fn(answer_text))  # expect ~[-1, 1]
    if tokenizer is not None:
        usage.scorer += _text_token_len_tok(tokenizer, answer_text)
    return v, usage


# ===========================
#  SBS (beam) with accounting
# ===========================


def sbs_decode2(
    mas: Any,
    question: str,
    score_type: str = "none",  # "none" | "prm" | "logprob"
    score_fn: Optional[Callable[[str], float]] = None,  # PRM for "prm"
    prm_tokenizer: Optional[Any] = None,  # tokenizer for PRM (token counting)
    B1: int = 1,
    B2: int = 5,
    gen_kwargs: Optional[Dict[str, Any]] = None,
    step_separator: str = "</step>",
    trace: Optional[List[Dict[str, Any]]] = None,
    return_trace: bool = False,
    *,
    logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
    final_answers_out: Optional[List[str]] = None,  # collect beam finals for pass@k
):

    usage = TokenStats()
    gen_kwargs = gen_kwargs or {}
    required_parents = {i: set(mas.parents.get(i, [])) for i in range(mas.n)}
    beams: List[Tuple[Dict[int, List[str]], float]] = [({}, 0.0)]
    local_trace: List[Dict[str, Any]] = [] if (trace is not None or return_trace) else None

    for agent_idx in range(mas.n):
        new_beams: List[Tuple[Dict[int, List[str]], float]] = []

        for traj, _ in beams:
            inbox, _, _ = _replay_trajectory(mas, question, traj)

            # prompt for this agent
            have = inbox.get(agent_idx, {})
            ordered_parents = sorted(required_parents[agent_idx])
            inputs = [have[p] for p in ordered_parents if p in have]
            sys_prompt = getattr(
                mas.agents[agent_idx], "system_prompt", "You are a helpful assistant."
            )
            msgs = [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": "\n\n".join(inputs).strip()},
            ]

            # PRE-ENCODE prompt once; reuse for accounting and logprob scoring
            enc = mas.agents[agent_idx].tok.apply_chat_template(
                msgs, tokenize=True, add_generation_prompt=True
            )
            p_len = (
                len(enc)
                if isinstance(enc, list)
                else (
                    int(
                        enc.numel()
                        if isinstance(enc, torch.Tensor) and enc.dim() == 1
                        else enc.shape[1]
                    )
                )
            )

            # Generate candidates
            cands = propose_agent_candidates(
                mas, agent_idx, inbox, required_parents, n_candidates=B2, **gen_kwargs
            )
            usage.agent_runs += B2

            # Account policy tokens (prompt + generated)
            usage.prompt += p_len
            for outs in cands:
                if outs:
                    usage.generated += _text_token_len_agent(mas.agents[agent_idx], outs[0])

            scored_cands = cands[:1] if score_type == "none" else cands
            cand_infos: List[Tuple[List[str], float]] = []

            for outs in scored_cands:
                if score_type == "prm":
                    traj2 = {**traj, agent_idx: outs}
                    s_text = render_state_text(mas, question, traj2, step_separator)
                    v = 0.0 if score_fn is None else float(score_fn(s_text))
                    if score_fn is not None:
                        usage.prm_calls += 1
                    if prm_tokenizer is not None:
                        usage.scorer += _text_token_len_tok(prm_tokenizer, s_text)
                elif score_type == "logprob":
                    s, sp, sg = compute_logprob_and_token_counts(
                        mas.agents[agent_idx], None, outs[0], prompt_ids=enc
                    )
                    usage.scorer += sp + sg
                    if logprob_agg in ("avg_token", "avg", "mean_token"):
                        v = s / max(1, sg)
                    elif logprob_agg in ("avg_step", "mean_step"):
                        # one step => same as "sum"
                        v = s
                    else:
                        v = s
                else:
                    v = 0.0
                cand_infos.append((outs, v))

            for outs, v in cand_infos:
                new_beams.append(({**traj, agent_idx: outs}, v))

            if local_trace is not None:
                local_trace.append(
                    {
                        "agent_idx": agent_idx,
                        "candidates": [
                            {
                                "text": (outs[0] if outs else ""),
                                "score": (float(v) if score_type != "none" else None),
                            }
                            for outs, v in cand_infos
                        ],
                        "chosen_text": None,
                        "chosen_score": None if score_type == "none" else float("-inf"),
                    }
                )

        if score_type != "none":
            new_beams.sort(key=lambda x: x[1], reverse=True)
        beams = new_beams[: max(1, B1)]

        if local_trace is not None and len(local_trace) > 0:
            chosen_traj, chosen_val = beams[0]
            chosen_text = chosen_traj.get(agent_idx, [""])[0]
            local_trace[-1]["chosen_text"] = chosen_text
            local_trace[-1]["chosen_score"] = None if score_type == "none" else float(chosen_val)

    # collect final answers for all surviving beams (for pass@k)
    if final_answers_out is not None:
        final_answers_out.clear()
        for traj, _ in beams:
            inbox_i, primary_out_i, last_i = _replay_trajectory(mas, question, traj)
            final_answers_out.append(_aggregate_final(mas, primary_out_i, last_i))

    best_traj, _ = beams[0]
    inbox, primary_out, last = _replay_trajectory(mas, question, best_traj)
    final_answer = _aggregate_final(mas, primary_out, last)

    if trace is not None:
        trace.extend(local_trace or [])
    if return_trace:
        return final_answer, usage, (local_trace or [])
    return final_answer, usage


# ===========================
#  Score-weighted voting
# ===========================


def _majority_by_numbers_equal(preds: List[str]) -> str:
    reps: List[str] = []
    counts: List[int] = []
    members: List[List[str]] = []
    for p in preds:
        placed = False
        for i, r in enumerate(reps):
            if numbers_equal(p, r):
                counts[i] += 1
                members[i].append(p)
                placed = True
                break
        if not placed:
            reps.append(p)
            counts.append(1)
            members.append([p])
    best_i = max(range(len(reps)), key=lambda i: (counts[i], -len(reps[i])))
    return min(members[best_i], key=len)


def _safe_exp(v: float) -> float:
    if v < -700:
        return 0.0
    if v > 700:
        return math.exp(700)
    return math.exp(v)


def _sigmoid(v: float) -> float:
    return 1.0 / (1.0 + _safe_exp(-v))


def make_scored_voter(
    base_decoder_with_trace: Callable[[Any, str], tuple[str, TokenStats, List[Dict[str, Any]]]],
    build_mas: Callable[[], Any],
    *,
    score_mode: str,  # "prm" | "logprob" | "orm"
    score_fn: Optional[Callable[[str], float]] = None,  # PRM or ORM
    prm_tokenizer=None,
    orm_tokenizer=None,
    step_separator: str = "</step>",
    k: int = 5,
    logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
    return_all_answers: bool = False,  # expose the k raw answers for pass@k
) -> Callable[[Any, str], Any]:
    """
    Runs k decodes with trace, groups predictions by numeric equivalence, and picks the
    group with the largest total *score weight* (PRM-product / sum-logprob / ORM-end).

    Returns either:
      - (chosen_answer, combined_usage)
      - or (chosen_answer, combined_usage, all_raw_answers) if return_all_answers=True.
    """

    def decoder(mas: Any, q: str) -> tuple[str, TokenStats]:
        usages = TokenStats()
        preds: List[tuple[str, float]] = []

        # First decode uses provided MAS; rest use fresh MAS
        mas_list = [mas] + [build_mas() for _ in range(max(0, k - 1))]
        for mas_i in mas_list:
            ans, u, tr = base_decoder_with_trace(mas_i, q)
            usages.add(u)
            steps = [e["chosen_text"] for e in tr] if tr else []

            if score_mode == "prm":
                w, u2 = score_path_prm_product(
                    score_fn,
                    mas_i,
                    q,
                    steps,
                    tokenizer=prm_tokenizer,
                    step_separator=step_separator,
                )
            elif score_mode == "logprob":
                raw_score, u2 = score_path_logprob(mas_i, q, steps, agg=logprob_agg)
                w = _safe_exp(raw_score)
            elif score_mode == "orm":
                raw_score, u2 = score_orm_end(score_fn, ans, tokenizer=orm_tokenizer)
                w = _sigmoid(raw_score)
            else:
                w, u2 = 1.0, TokenStats()
            usages.add(u2)
            preds.append((ans, w))

        # Group by numeric equivalence and sum weights
        clusters: List[Dict[str, Any]] = []
        for ans, w in preds:
            for c in clusters:
                if numbers_equal(ans, c["rep"]):
                    c["weight"] += w
                    c["members"].append((ans, w))
                    break
            else:
                clusters.append({"rep": ans, "weight": w, "members": [(ans, w)]})

        best = max(clusters, key=lambda c: (c["weight"], len(c["members"]), -len(c["rep"])))
        chosen = min((a for a, _ in best["members"]), key=len)
        raw_answers = [ans for ans, _ in preds]

        if return_all_answers:
            return chosen, usages, raw_answers
        return chosen, usages

    return decoder


# ===========================
#   Model / tokenizer I/O
# ===========================


def load_prm_scorer(
    prm_dir: str,
    base_model_id: str = None,
    max_length: int = 2048,
    torch_dtype: torch.dtype = torch.float16,
):
    """
    Returns: (score: Callable[[str], float], tokenizer, model)
    Loads a token-classification base + LoRA PRM adapter.
    Score = tanh(last-token-logit) in [-1, 1].
    """
    # Resolve base model if not supplied
    if base_model_id is None:
        cfg = PeftConfig.from_pretrained(prm_dir)
        base_model_id = getattr(cfg, "base_model_name_or_path", None) or "Qwen/Qwen2.5-7B-Instruct"

    # Tokenizer: prefer saved with adapter; else base
    tok_src = prm_dir if (Path(prm_dir) / "tokenizer.json").exists() else base_model_id
    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True, trust_remote_code=True)
    if tok.pad_token is None and tok.eos_token is not None:
        tok.pad_token = tok.eos_token

    # Base model with PRM head
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
    )

    base = AutoModelForTokenClassification.from_pretrained(
        base_model_id,
        num_labels=1,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_cfg,
    )

    # Align embeddings to tokenizer before loading adapter
    if len(tok) != base.get_input_embeddings().weight.size(0):
        base.resize_token_embeddings(len(tok))

    # Attach LoRA adapter
    model = PeftModel.from_pretrained(base, prm_dir)
    model.eval()
    device = next(model.parameters()).device

    @torch.inference_mode()
    def score(text: str) -> float:
        inputs = tok(text, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        out = model(**inputs)  # logits: (1, T, 1)
        z = out.logits[0, -1, 0].float()
        return float(torch.tanh(z).item())

    return score, tok, model


def load_generation_model(
    base_model_id: str,
    tokenizer: Optional[AutoTokenizer] = None,
    torch_dtype: torch.dtype = torch.float16,
):
    """
    Loads a Causal LM for generation. If `tokenizer` provided, embeddings are resized accordingly.
    """
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_id, use_fast=True, trust_remote_code=True
        )
        if tokenizer.pad_token is None and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch_dtype,
        device_map="auto",
        trust_remote_code=True,
    )
    if len(tokenizer) != model.get_input_embeddings().weight.size(0):
        model.resize_token_embeddings(len(tokenizer))
    model.eval()
    return model, tokenizer


def ensure_separator_token(tokenizer: AutoTokenizer, sep: str) -> int:
    """
    Adds `sep` to vocab as additional special token if missing. Returns 0/1 tokens added.
    """
    added = 0
    added_vocab = tokenizer.get_added_vocab()
    if sep not in added_vocab:
        added += tokenizer.add_special_tokens({"additional_special_tokens": [sep]})
        if tokenizer.pad_token is None and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
    return added


def align_model_to_tokenizer(model, tokenizer) -> None:
    """Resize model embeddings to match tokenizer size if needed."""
    if len(tokenizer) != model.get_input_embeddings().weight.size(0):
        model.resize_token_embeddings(len(tokenizer))


def render_state_text(
    mas: Any,
    question: str,
    trajectory: Dict[int, List[str]],
    step_separator: str = "</step>",
) -> str:
    """Question + chosen primary outputs so far, joined with separator."""
    parts = [question]
    for j in range(mas.n):
        if j in trajectory and trajectory[j]:
            parts.append(trajectory[j][0])
        else:
            break
    return step_separator.join(parts)


# =========================================================
#                       MCTS inference
# =========================================================


@dataclass
class _Node:
    traj: Dict[int, List[str]] = field(default_factory=dict)
    children: List["_Node"] = field(default_factory=list)
    visits: int = 0
    q_sum: float = 0.0
    v_init: float = 0.0
    action_text: str = ""
    prior: float = 0.0

    @property
    def q_mean(self) -> float:
        return (self.q_sum + self.v_init) / (self.visits + 1)


class MCTSInfer:
    """
    UCT inference with pluggable scoring:
      - "prm": V(s) from PRM on state text (step prefixes)
      - "logprob": value from policy logprob (per-step; path-level at leaf)
      - "orm": value only at terminal from ORM (end-of-answer)
    Tracks token usage (prompt, generated, scorer).
    """

    def __init__(
        self,
        mas: MAS,
        question: str,
        score_fn: Optional[Callable[[str], float]] = None,  # PRM or ORM
        *,
        score_type: str = "prm",  # "prm" | "logprob" | "orm"
        prm_tokenizer: Optional[Any] = None,
        orm_tokenizer: Optional[Any] = None,
        max_children: int = 5,
        c_uct: float = 2.0,
        gen_kwargs: Optional[Dict[str, Any]] = None,
        step_separator: str = "</step>",
        uct_type: str = "uct",  #  "uct" | "puct"
        leaf_score_type: Optional[str] = None,  # override leaf eval, e.g., "orm"
        logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
        leaf_score_fn: Optional[Callable[[str], float]] = None,  # leaf PRM/ORM
    ):
        self.mas = mas
        self.q = question
        self.score_fn = score_fn
        self.score_type = score_type
        self.prm_tokenizer = prm_tokenizer
        self.orm_tokenizer = orm_tokenizer
        self.max_children = int(max(1, max_children))
        self.c = float(c_uct)
        self.kw = gen_kwargs or {}
        self.sep = step_separator
        self.leaf_score_fn = leaf_score_fn or score_fn

        self.uct_type = (uct_type or "uct").lower()
        self.leaf = (leaf_score_type or "").lower() or None
        self.logprob_agg = (logprob_agg or "sum").lower()

        self.required_parents = {i: set(self.mas.parents.get(i, [])) for i in range(self.mas.n)}
        self.parent_order = {i: sorted(self.required_parents[i]) for i in range(self.mas.n)}
        self.sys_prompts = [
            getattr(self.mas.agents[i], "system_prompt", "You are a helpful assistant.")
            for i in range(self.mas.n)
        ]
        self.root = _Node()
        self.usage = TokenStats()

    def _depth(self, node: _Node) -> int:
        return len(node.traj)

    def _terminal(self, node: _Node) -> bool:
        return self._depth(node) >= self.mas.n

    def _uct(self, parent: _Node, child: _Node) -> float:
        Np = max(parent.visits + 1, 1)
        Nc = child.visits + 1
        return child.q_mean + self.c * math.sqrt(math.log(Np) / Nc)

    def _puct(self, parent: _Node, child: _Node) -> float:
        Np = max(parent.visits, 1)
        Nc = child.visits
        return child.q_mean + self.c * child.prior * math.sqrt(Np) / (1.0 + Nc)

    def _select_score(self, parent: _Node, child: _Node) -> float:
        if self.uct_type == "puct":
            return self._puct(parent, child)
        return self._uct(parent, child)

    def _state_text(self, traj: Dict[int, List[str]]) -> str:
        return render_state_text(self.mas, self.q, traj, self.sep)

    def _expand(self, node: _Node) -> None:
        d = self._depth(node)
        if d >= self.mas.n:
            return

        inbox, _, _ = _replay_trajectory(self.mas, self.q, node.traj)
        have = inbox.get(d, {})
        inputs = [have[p] for p in self.parent_order[d] if p in have]
        msgs = [
            {"role": "system", "content": self.sys_prompts[d]},
            {"role": "user", "content": "\n\n".join(inputs).strip()},
        ]
        enc = self.mas.agents[d].tok.apply_chat_template(
            msgs, tokenize=True, add_generation_prompt=True
        )
        p_len = (
            len(enc)
            if isinstance(enc, list)
            else (
                int(
                    enc.numel()
                    if isinstance(enc, torch.Tensor) and enc.dim() == 1
                    else enc.shape[1]
                )
            )
        )

        candidates = propose_agent_candidates(
            self.mas, d, inbox, self.required_parents, n_candidates=self.max_children, **self.kw
        )
        # Deduplicate identical candidate actions (same text => same action).
        dedup = []
        seen = set()
        for outs in candidates:
            t = outs[0] if outs else ""
            if t in seen:
                continue
            seen.add(t)
            dedup.append(outs)
        candidates = dedup

        self.usage.agent_runs += self.max_children

        self.usage.prompt += p_len
        for outs in candidates:
            if outs:
                self.usage.generated += _text_token_len_agent(self.mas.agents[d], outs[0])

        vals: List[float] = []
        child_trajs: List[Dict[int, List[str]]] = []
        child_texts: List[str] = []

        # Compute values for each candidate
        for outs in candidates:
            traj2 = {**node.traj, d: outs}
            child_trajs.append(traj2)
            child_texts.append(outs[0] if outs else "")

            if self.score_type == "prm":
                v = 0.0 if self.score_fn is None else float(self.score_fn(self._state_text(traj2)))

                if self.score_fn is not None:
                    self.usage.prm_calls += 1
                if self.prm_tokenizer is not None:
                    self.usage.scorer += _text_token_len_tok(
                        self.prm_tokenizer, self._state_text(traj2)
                    )

            elif self.score_type == "logprob":
                v_lp, _, g_len = compute_logprob_and_token_counts(
                    self.mas.agents[d], None, outs[0], prompt_ids=enc
                )
                self.usage.scorer += p_len + g_len
                if self.logprob_agg in ("avg_token", "avg", "mean_token"):
                    v = v_lp / max(1, g_len)
                else:  # "sum" or "avg_step" (per-step = sum here)
                    v = v_lp

            else:  # "orm" does not score partials
                v = 0.0

            vals.append(v)

        # 2) compute priors for PUCT (softmax over v_init); safe even if UCT
        priors = [1.0 / len(candidates)] * len(candidates) if candidates else []
        if self.uct_type == "puct" and candidates:
            can_logprob = getattr(self.mas.agents[d], "model", None) is not None
            if can_logprob:
                try:
                    logpriors = []
                    for outs in candidates:
                        text = outs[0] if outs else ""
                        lp, sp, sg = compute_logprob_and_token_counts(
                            self.mas.agents[d], None, text, prompt_ids=enc
                        )
                        self.usage.scorer += sp + sg
                        logpriors.append(lp / max(1, sg))

                    m = max(logpriors)
                    exps = [math.exp(lp - m) for lp in logpriors]
                    Z = sum(exps) if exps else 1.0
                    priors = [e / Z for e in exps]
                except Exception:
                    pass  # keep uniform

        # 3) create children with v_init and prior
        for v, traj2, a_text, p0 in zip(vals, child_trajs, child_texts, priors):
            node.children.append(
                _Node(
                    traj=traj2,
                    children=[],
                    visits=0,
                    q_sum=0.0,
                    v_init=v,  # keep as cached eval if you want, but not as a fake visit
                    action_text=a_text,
                    prior=p0,
                )
            )

    def rollout(self) -> None:
        node = self.root
        path = [node]

        while not self._terminal(node):
            if not node.children:
                self._expand(node)
                if not node.children:
                    break
                node = max(node.children, key=lambda ch: self._select_score(path[-1], ch))
                path.append(node)
            else:
                node = max(node.children, key=lambda ch: self._select_score(path[-1], ch))
                path.append(node)

        leaf_traj = path[-1].traj
        leaf_type = self.leaf or self.score_type

        if leaf_type == "prm":
            v_leaf = path[-1].v_init  # cached score for this node

        elif leaf_type == "logprob":
            steps = [leaf_traj[i][0] for i in range(len(leaf_traj)) if leaf_traj.get(i)]
            v_leaf, u = score_path_logprob(self.mas, self.q, steps, agg=self.logprob_agg)
            self.usage.add(u)

        else:  # "orm"
            inbox, primary_out, last = _replay_trajectory(self.mas, self.q, leaf_traj)
            ans = _aggregate_final(self.mas, primary_out, last)
            v_leaf, u = score_orm_end(
                self.leaf_score_fn, ans, tokenizer=self.orm_tokenizer
            )  # use leaf scorer
            self.usage.add(u)
            self.usage.prm_calls += 1

        for n in path:
            n.visits += 1
            n.q_sum += v_leaf

    def decode(self, n_simulations: int = 40) -> Tuple[str, TokenStats]:
        for _ in range(n_simulations):
            self.rollout()
        traj = {}
        node = self.root
        while not self._terminal(node) and node.children:
            node = max(node.children, key=lambda ch: ch.q_mean)
            traj = node.traj

        inbox, primary_out, last = _replay_trajectory(self.mas, self.q, traj)
        return _aggregate_final(self.mas, primary_out, last), self.usage

    def get_topk_answers(self, k: int = 5) -> List[str]:
        """
        Collect up to k final answers from the current MCTS tree, ranked by node q_mean.
        This only replays stored trajectories (no extra model calls).
        """
        if k <= 0:
            return []

        candidates: List[Tuple[float, str]] = []
        stack = [self.root]
        while stack:
            node = stack.pop()
            # Use nodes that can serve as final states (leaf or terminal) and that have a trajectory.
            if node.traj and self._terminal(node) and node.visits > 0:
                inbox, primary_out, last = _replay_trajectory(self.mas, self.q, node.traj)
                ans = _aggregate_final(self.mas, primary_out, last)
                candidates.append((node.q_mean, ans))
            stack.extend(node.children)

        if not candidates:
            return []

        candidates.sort(key=lambda x: x[0], reverse=True)
        return [ans for _, ans in candidates[:k]]
