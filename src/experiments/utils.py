from dataclasses import dataclass, field
from typing import Optional, Callable, Dict, List, Tuple, Any
import os
import random

import math
import statistics
from pathlib import Path

import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
from tqdm import tqdm
from transformers import BitsAndBytesConfig
import ray
from ray.util.queue import Queue
from openai import OpenAI
import queue

# Project imports
from mcts import (
    is_correct,
    numbers_equal,
    MAS,
    propose_agent_candidates,
    _replay_trajectory,
    _aggregate_final,
)

from agent import Agent


# =========================================================
#                      Token accounting
# =========================================================


def _seed_everything(seed: int) -> None:
    try:
        os.environ["PYTHONHASHSEED"] = str(int(seed))
    except Exception:
        pass
    random.seed(int(seed))
    try:
        np.random.seed(int(seed))
    except Exception:
        pass
    torch.manual_seed(int(seed))


@dataclass
class TokenStats:
    prompt: int = 0  # tokens in prompts sent to the policy LM
    generated: int = 0  # tokens generated by the policy LM
    scorer: int = 0  # tokens processed by scoring models (PRM/logprob/ORM)
    prm_calls: int = 0  # how many times PRM was called
    agent_runs: int = 0  # how many times we ran a single agent (one step decode)

    def total(self) -> int:
        return self.prompt + self.generated + self.scorer

    def add(self, other: "TokenStats") -> "TokenStats":
        self.prompt += other.prompt
        self.generated += other.generated
        self.scorer += other.scorer
        self.prm_calls += other.prm_calls
        self.agent_runs += other.agent_runs
        return self


# ======================
#  Token length helpers
# ======================


def _text_token_len_tok(tok, text: str) -> int:
    return int(tok(text, return_tensors="pt", add_special_tokens=False)["input_ids"].shape[1])


def _text_token_len_agent(agent, text: str) -> int:
    return _text_token_len_tok(agent.tok, text)


# ===========================
#  LOGPROB scoring utilities
# ===========================


@torch.inference_mode()
def compute_logprob_and_token_counts(
    agent: Agent,
    msgs=None,
    completion: str = "",
    *,
    prompt_ids=None,  # pass pre-encoded prompt to avoid re-tokenization
) -> tuple[float, int, int]:
    """
    Returns (sum_logprob, prompt_len, gen_len).
    Vectorized; avoids Python loops. If `prompt_ids` is provided (list[int] or Tensor),
    `msgs` is ignored.
    """
    tok = agent.tok
    model = agent.model
    device = next(model.parameters()).device

    # Resolve prompt_ids (B=1)
    if prompt_ids is None:
        enc = tok.apply_chat_template(msgs, tokenize=True, add_generation_prompt=True)
        if isinstance(enc, list):
            prompt_ids = torch.tensor(enc, dtype=torch.long).unsqueeze(0).to(device)
        elif isinstance(enc, torch.Tensor):
            prompt_ids = (enc.unsqueeze(0) if enc.dim() == 1 else enc).to(device)
        else:
            s = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            prompt_ids = tok(s, return_tensors="pt", add_special_tokens=False)["input_ids"].to(
                device
            )
    else:
        if isinstance(prompt_ids, list):
            prompt_ids = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0).to(device)
        elif isinstance(prompt_ids, torch.Tensor):
            prompt_ids = (prompt_ids.unsqueeze(0) if prompt_ids.dim() == 1 else prompt_ids).to(
                device
            )
        else:
            raise TypeError("prompt_ids must be list[int] or torch.Tensor")

    comp_ids = tok(completion, return_tensors="pt", add_special_tokens=False)["input_ids"].to(
        device
    )
    input_ids = torch.cat([prompt_ids, comp_ids], dim=1)  # [1, L]
    attn = torch.ones_like(input_ids, device=device)

    out = model(input_ids=input_ids, attention_mask=attn)
    logits = out.logits[:, :-1, :]  # next-token logits, [1, L-1, V]
    target = input_ids[:, 1:]  # tokens to predict, [1, L-1]

    prompt_len = int(prompt_ids.shape[1])
    gen_len = int(comp_ids.shape[1])
    start = max(0, prompt_len - 1)  # first position predicting gen token 0
    end = start + gen_len

    # Vectorized gather of logprobs for generated tokens
    lp_slice = F.log_softmax(logits[0, start:end, :], dim=-1)  # [g, V]
    tgt_slice = target[0, start:end].unsqueeze(-1)  # [g, 1]
    s = lp_slice.gather(-1, tgt_slice).sum().item()  # scalar

    return float(s), prompt_len, gen_len


def score_path_logprob(
    mas,
    question: str,
    steps: list[str],
    *,
    agg: str = "sum",
) -> tuple[float, TokenStats]:
    required_parents = {i: set(mas.parents.get(i, [])) for i in range(mas.n)}
    usage = TokenStats()
    traj_so_far = {}
    s_total = 0.0
    token_total = 0

    for j, text in enumerate(steps):
        inbox, _, _ = _replay_trajectory(mas, question, traj_so_far)
        have = inbox.get(j, {})
        ordered_parents = sorted(required_parents[j])
        inputs = [have[p] for p in ordered_parents if p in have]
        sys_prompt = getattr(mas.agents[j], "system_prompt", "You are a helpful assistant.")
        msgs = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": "\n\n".join(inputs).strip()},
        ]

        lp, p_len, g_len = compute_logprob_and_token_counts(mas.agents[j], msgs, text)
        usage.scorer += p_len + g_len
        s_total += lp
        token_total += max(0, g_len)

        traj_so_far[j] = [text]

    if agg in ("avg_token", "avg", "mean_token"):
        value = s_total / max(1, token_total)
    elif agg in ("avg_step", "mean_step"):
        value = s_total / max(1, len(steps))
    else:  # "sum" (default)
        value = s_total

    return value, usage


def score_path_prm_product(
    score_fn, mas, question: str, steps: list[str], tokenizer=None, step_separator: str = "</step>"
) -> tuple[float, TokenStats]:
    usage = TokenStats()
    traj = {}
    product = 1.0
    for j, text in enumerate(steps):
        traj[j] = [text]
        s_text = render_state_text(mas, question, traj, step_separator)
        v = float(score_fn(s_text))  # [-1, 1]
        usage.prm_calls += 1
        w = (v + 1.0) / 2.0
        product *= max(1e-6, w)
        if tokenizer is not None:
            usage.scorer += _text_token_len_tok(tokenizer, s_text)
    return product, usage


def score_orm_end(orm_fn, answer_text: str, tokenizer=None) -> tuple[float, TokenStats]:
    usage = TokenStats()
    v = float(orm_fn(answer_text))  # expect ~[-1, 1]
    if tokenizer is not None:
        usage.scorer += _text_token_len_tok(tokenizer, answer_text)
    return v, usage


# ===========================
#  SBS (beam) with accounting
# ===========================


def sbs_decode2(
    mas: Any,
    question: str,
    score_type: str = "none",  # "none" | "prm" | "logprob"
    score_fn: Optional[Callable[[str], float]] = None,  # PRM for "prm"
    prm_tokenizer: Optional[Any] = None,  # tokenizer for PRM (token counting)
    B1: int = 1,
    B2: int = 5,
    gen_kwargs: Optional[Dict[str, Any]] = None,
    step_separator: str = "</step>",
    trace: Optional[List[Dict[str, Any]]] = None,
    return_trace: bool = False,
    *,
    logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
    final_answers_out: Optional[List[str]] = None,  # collect beam finals for pass@k
):

    usage = TokenStats()
    gen_kwargs = gen_kwargs or {}
    required_parents = {i: set(mas.parents.get(i, [])) for i in range(mas.n)}
    beams: List[Tuple[Dict[int, List[str]], float]] = [({}, 0.0)]
    local_trace: List[Dict[str, Any]] = [] if (trace is not None or return_trace) else None

    for agent_idx in range(mas.n):
        new_beams: List[Tuple[Dict[int, List[str]], float]] = []

        for traj, _ in beams:
            inbox, _, _ = _replay_trajectory(mas, question, traj)

            # prompt for this agent
            have = inbox.get(agent_idx, {})
            ordered_parents = sorted(required_parents[agent_idx])
            inputs = [have[p] for p in ordered_parents if p in have]
            sys_prompt = getattr(
                mas.agents[agent_idx], "system_prompt", "You are a helpful assistant."
            )
            msgs = [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": "\n\n".join(inputs).strip()},
            ]

            # PRE-ENCODE prompt once; reuse for accounting and logprob scoring
            enc = mas.agents[agent_idx].tok.apply_chat_template(
                msgs, tokenize=True, add_generation_prompt=True
            )
            p_len = (
                len(enc)
                if isinstance(enc, list)
                else (
                    int(
                        enc.numel()
                        if isinstance(enc, torch.Tensor) and enc.dim() == 1
                        else enc.shape[1]
                    )
                )
            )

            # Generate candidates
            cands = propose_agent_candidates(
                mas, agent_idx, inbox, required_parents, n_candidates=B2, **gen_kwargs
            )
            usage.agent_runs += B2

            # Account policy tokens (prompt + generated)
            usage.prompt += p_len
            for outs in cands:
                if outs:
                    usage.generated += _text_token_len_agent(mas.agents[agent_idx], outs[0])

            scored_cands = cands[:1] if score_type == "none" else cands
            cand_infos: List[Tuple[List[str], float]] = []

            for outs in scored_cands:
                if score_type == "prm":
                    traj2 = {**traj, agent_idx: outs}
                    s_text = render_state_text(mas, question, traj2, step_separator)
                    v = 0.0 if score_fn is None else float(score_fn(s_text))
                    if score_fn is not None:
                        usage.prm_calls += 1
                    if prm_tokenizer is not None:
                        usage.scorer += _text_token_len_tok(prm_tokenizer, s_text)
                elif score_type == "logprob":
                    s, sp, sg = compute_logprob_and_token_counts(
                        mas.agents[agent_idx], None, outs[0], prompt_ids=enc
                    )
                    usage.scorer += sp + sg
                    if logprob_agg in ("avg_token", "avg", "mean_token"):
                        v = s / max(1, sg)
                    elif logprob_agg in ("avg_step", "mean_step"):
                        # one step => same as "sum"
                        v = s
                    else:
                        v = s
                else:
                    v = 0.0
                cand_infos.append((outs, v))

            for outs, v in cand_infos:
                new_beams.append(({**traj, agent_idx: outs}, v))

            if local_trace is not None:
                local_trace.append(
                    {
                        "agent_idx": agent_idx,
                        "candidates": [
                            {
                                "text": (outs[0] if outs else ""),
                                "score": (float(v) if score_type != "none" else None),
                            }
                            for outs, v in cand_infos
                        ],
                        "chosen_text": None,
                        "chosen_score": None if score_type == "none" else float("-inf"),
                    }
                )

        if score_type != "none":
            new_beams.sort(key=lambda x: x[1], reverse=True)
        beams = new_beams[: max(1, B1)]

        if local_trace is not None and len(local_trace) > 0:
            chosen_traj, chosen_val = beams[0]
            chosen_text = chosen_traj.get(agent_idx, [""])[0]
            local_trace[-1]["chosen_text"] = chosen_text
            local_trace[-1]["chosen_score"] = None if score_type == "none" else float(chosen_val)

    # collect final answers for all surviving beams (for pass@k)
    if final_answers_out is not None:
        final_answers_out.clear()
        for traj, _ in beams:
            inbox_i, primary_out_i, last_i = _replay_trajectory(mas, question, traj)
            final_answers_out.append(_aggregate_final(mas, primary_out_i, last_i))

    best_traj, _ = beams[0]
    inbox, primary_out, last = _replay_trajectory(mas, question, best_traj)
    final_answer = _aggregate_final(mas, primary_out, last)

    if trace is not None:
        trace.extend(local_trace or [])
    if return_trace:
        return final_answer, usage, (local_trace or [])
    return final_answer, usage


# ===========================
#  Score-weighted voting
# ===========================


def _majority_by_numbers_equal(preds: List[str]) -> str:
    reps: List[str] = []
    counts: List[int] = []
    members: List[List[str]] = []
    for p in preds:
        placed = False
        for i, r in enumerate(reps):
            if numbers_equal(p, r):
                counts[i] += 1
                members[i].append(p)
                placed = True
                break
        if not placed:
            reps.append(p)
            counts.append(1)
            members.append([p])
    best_i = max(range(len(reps)), key=lambda i: (counts[i], -len(reps[i])))
    return min(members[best_i], key=len)


def make_majority_voter(
    base_decoder: Callable[[Any, str], str], build_mas: Callable[[], Any], k: int = 5
) -> Callable[[Any, str], str]:
    def decoder(mas: Any, q: str) -> str:
        preds: List[str] = [base_decoder(mas, q)]
        for _ in range(max(0, k - 1)):
            preds.append(base_decoder(build_mas(), q))
        return _majority_by_numbers_equal(preds)

    return decoder


def _safe_exp(v: float) -> float:
    if v < -700:
        return 0.0
    if v > 700:
        return math.exp(700)
    return math.exp(v)


def _sigmoid(v: float) -> float:
    return 1.0 / (1.0 + _safe_exp(-v))


def make_scored_voter(
    base_decoder_with_trace: Callable[[Any, str], tuple[str, TokenStats, List[Dict[str, Any]]]],
    build_mas: Callable[[], Any],
    *,
    score_mode: str,  # "prm" | "logprob" | "orm"
    score_fn: Optional[Callable[[str], float]] = None,  # PRM or ORM
    prm_tokenizer=None,
    orm_tokenizer=None,
    step_separator: str = "</step>",
    k: int = 5,
    logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
    return_all_answers: bool = False,  # expose the k raw answers for pass@k
) -> Callable[[Any, str], Any]:
    """
    Runs k decodes with trace, groups predictions by numeric equivalence, and picks the
    group with the largest total *score weight* (PRM-product / sum-logprob / ORM-end).

    Returns either:
      - (chosen_answer, combined_usage)
      - or (chosen_answer, combined_usage, all_raw_answers) if return_all_answers=True.
    """

    def decoder(mas: Any, q: str) -> tuple[str, TokenStats]:
        usages = TokenStats()
        preds: List[tuple[str, float]] = []

        # First decode uses provided MAS; rest use fresh MAS
        mas_list = [mas] + [build_mas() for _ in range(max(0, k - 1))]
        for mas_i in mas_list:
            ans, u, tr = base_decoder_with_trace(mas_i, q)
            usages.add(u)
            steps = [e["chosen_text"] for e in tr] if tr else []

            if score_mode == "prm":
                w, u2 = score_path_prm_product(
                    score_fn,
                    mas_i,
                    q,
                    steps,
                    tokenizer=prm_tokenizer,
                    step_separator=step_separator,
                )
            elif score_mode == "logprob":
                raw_score, u2 = score_path_logprob(mas_i, q, steps, agg=logprob_agg)
                w = _safe_exp(raw_score)
            elif score_mode == "orm":
                raw_score, u2 = score_orm_end(score_fn, ans, tokenizer=orm_tokenizer)
                w = _sigmoid(raw_score)
            else:
                w, u2 = 1.0, TokenStats()
            usages.add(u2)
            preds.append((ans, w))

        # Group by numeric equivalence and sum weights
        clusters: List[Dict[str, Any]] = []
        for ans, w in preds:
            for c in clusters:
                if numbers_equal(ans, c["rep"]):
                    c["weight"] += w
                    c["members"].append((ans, w))
                    break
            else:
                clusters.append({"rep": ans, "weight": w, "members": [(ans, w)]})

        best = max(clusters, key=lambda c: (c["weight"], len(c["members"]), -len(c["rep"])))
        chosen = min((a for a, _ in best["members"]), key=len)
        raw_answers = [ans for ans, _ in preds]

        if return_all_answers:
            return chosen, usages, raw_answers
        return chosen, usages

    return decoder


# ===========================
#   Model / tokenizer I/O
# ===========================


def load_prm_scorer(
    prm_dir: str,
    base_model_id: str = None,
    max_length: int = 2048,
    torch_dtype: torch.dtype = torch.float16,
):
    """
    Returns: (score: Callable[[str], float], tokenizer, model)
    Loads a token-classification base + LoRA PRM adapter.
    Score = tanh(last-token-logit) in [-1, 1].
    """
    # Resolve base model if not supplied
    if base_model_id is None:
        cfg = PeftConfig.from_pretrained(prm_dir)
        base_model_id = getattr(cfg, "base_model_name_or_path", None) or "Qwen/Qwen2.5-7B-Instruct"

    # Tokenizer: prefer saved with adapter; else base
    tok_src = prm_dir if (Path(prm_dir) / "tokenizer.json").exists() else base_model_id
    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True, trust_remote_code=True)
    if tok.pad_token is None and tok.eos_token is not None:
        tok.pad_token = tok.eos_token

    # Base model with PRM head
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
    )

    base = AutoModelForTokenClassification.from_pretrained(
        base_model_id,
        num_labels=1,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_cfg,
    )

    # Align embeddings to tokenizer before loading adapter
    if len(tok) != base.get_input_embeddings().weight.size(0):
        base.resize_token_embeddings(len(tok))

    # Attach LoRA adapter
    model = PeftModel.from_pretrained(base, prm_dir)
    model.eval()
    device = next(model.parameters()).device

    @torch.inference_mode()
    def score(text: str) -> float:
        inputs = tok(text, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        out = model(**inputs)  # logits: (1, T, 1)
        z = out.logits[0, -1, 0].float()
        return float(torch.tanh(z).item())

    return score, tok, model


def load_generation_model(
    base_model_id: str,
    tokenizer: Optional[AutoTokenizer] = None,
    torch_dtype: torch.dtype = torch.float16,
):
    """
    Loads a Causal LM for generation. If `tokenizer` provided, embeddings are resized accordingly.
    """
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_id, use_fast=True, trust_remote_code=True
        )
        if tokenizer.pad_token is None and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch_dtype,
        device_map="auto",
        trust_remote_code=True,
    )
    if len(tokenizer) != model.get_input_embeddings().weight.size(0):
        model.resize_token_embeddings(len(tokenizer))
    model.eval()
    return model, tokenizer


def ensure_separator_token(tokenizer: AutoTokenizer, sep: str) -> int:
    """
    Adds `sep` to vocab as additional special token if missing. Returns 0/1 tokens added.
    """
    added = 0
    added_vocab = tokenizer.get_added_vocab()
    if sep not in added_vocab:
        added += tokenizer.add_special_tokens({"additional_special_tokens": [sep]})
        if tokenizer.pad_token is None and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
    return added


def align_model_to_tokenizer(model, tokenizer) -> None:
    """Resize model embeddings to match tokenizer size if needed."""
    if len(tokenizer) != model.get_input_embeddings().weight.size(0):
        model.resize_token_embeddings(len(tokenizer))


def render_state_text(
    mas: Any,
    question: str,
    trajectory: Dict[int, List[str]],
    step_separator: str = "</step>",
) -> str:
    """Question + chosen primary outputs so far, joined with separator."""
    parts = [question]
    for j in range(mas.n):
        if j in trajectory and trajectory[j]:
            parts.append(trajectory[j][0])
        else:
            break
    return step_separator.join(parts)


def build_mas_from_specs(model, tok, agent_specs: List[Dict[str, Any]], edges: List[List[int]]):
    """Shared MAS builder from config specs."""
    agents = []
    for spec in agent_specs:
        agents.append(
            Agent(
                model,
                tok,
                system_prompt=spec.get("system_prompt", ""),
                max_new_tokens=int(spec.get("max_new_tokens", 512)),
            )
        )
    return MAS(edges, agents)


# =========================================================
#                       MCTS inference
# =========================================================


@dataclass
class _Node:
    traj: Dict[int, List[str]] = field(default_factory=dict)
    children: List["_Node"] = field(default_factory=list)
    visits: int = 0
    q_sum: float = 0.0
    v_init: float = 0.0
    action_text: str = ""
    prior: float = 0.0

    @property
    def q_mean(self) -> float:
        return (self.q_sum + self.v_init) / (self.visits + 1)


class MCTSInfer:
    """
    UCT inference with pluggable scoring:
      - "prm": V(s) from PRM on state text (step prefixes)
      - "logprob": value from policy logprob (per-step; path-level at leaf)
      - "orm": value only at terminal from ORM (end-of-answer)
    Tracks token usage (prompt, generated, scorer).
    """

    def __init__(
        self,
        mas: MAS,
        question: str,
        score_fn: Optional[Callable[[str], float]] = None,  # PRM or ORM
        *,
        score_type: str = "prm",  # "prm" | "logprob" | "orm"
        prm_tokenizer: Optional[Any] = None,
        orm_tokenizer: Optional[Any] = None,
        max_children: int = 5,
        c_uct: float = 2.0,
        gen_kwargs: Optional[Dict[str, Any]] = None,
        step_separator: str = "</step>",
        uct_type: str = "uct",  #  "uct" | "puct"
        leaf_score_type: Optional[str] = None,  # override leaf eval, e.g., "orm"
        logprob_agg: str = "sum",  # "sum" | "avg_token" | "avg_step"
        leaf_score_fn: Optional[Callable[[str], float]] = None,  # leaf PRM/ORM
    ):
        self.mas = mas
        self.q = question
        self.score_fn = score_fn
        self.score_type = score_type
        self.prm_tokenizer = prm_tokenizer
        self.orm_tokenizer = orm_tokenizer
        self.max_children = int(max(1, max_children))
        self.c = float(c_uct)
        self.kw = gen_kwargs or {}
        self.sep = step_separator
        self.leaf_score_fn = leaf_score_fn or score_fn

        self.uct_type = (uct_type or "uct").lower()
        self.leaf = (leaf_score_type or "").lower() or None
        self.logprob_agg = (logprob_agg or "sum").lower()

        self.required_parents = {i: set(self.mas.parents.get(i, [])) for i in range(self.mas.n)}
        self.parent_order = {i: sorted(self.required_parents[i]) for i in range(self.mas.n)}
        self.sys_prompts = [
            getattr(self.mas.agents[i], "system_prompt", "You are a helpful assistant.")
            for i in range(self.mas.n)
        ]
        self.root = _Node()
        self.usage = TokenStats()

    def _depth(self, node: _Node) -> int:
        return len(node.traj)

    def _terminal(self, node: _Node) -> bool:
        return self._depth(node) >= self.mas.n

    def _uct(self, parent: _Node, child: _Node) -> float:
        Np = max(parent.visits + 1, 1)
        Nc = child.visits + 1
        return child.q_mean + self.c * math.sqrt(math.log(Np) / Nc)

    def _puct(self, parent: _Node, child: _Node) -> float:
        Np = max(parent.visits, 1)
        Nc = child.visits
        return child.q_mean + self.c * child.prior * math.sqrt(Np) / (1.0 + Nc)

    def _select_score(self, parent: _Node, child: _Node) -> float:
        if self.uct_type == "puct":
            return self._puct(parent, child)
        return self._uct(parent, child)

    def _state_text(self, traj: Dict[int, List[str]]) -> str:
        return render_state_text(self.mas, self.q, traj, self.sep)

    def _expand(self, node: _Node) -> None:
        d = self._depth(node)
        if d >= self.mas.n:
            return

        inbox, _, _ = _replay_trajectory(self.mas, self.q, node.traj)
        have = inbox.get(d, {})
        inputs = [have[p] for p in self.parent_order[d] if p in have]
        msgs = [
            {"role": "system", "content": self.sys_prompts[d]},
            {"role": "user", "content": "\n\n".join(inputs).strip()},
        ]
        enc = self.mas.agents[d].tok.apply_chat_template(
            msgs, tokenize=True, add_generation_prompt=True
        )
        p_len = (
            len(enc)
            if isinstance(enc, list)
            else (
                int(
                    enc.numel()
                    if isinstance(enc, torch.Tensor) and enc.dim() == 1
                    else enc.shape[1]
                )
            )
        )

        candidates = propose_agent_candidates(
            self.mas, d, inbox, self.required_parents, n_candidates=self.max_children, **self.kw
        )
        # Deduplicate identical candidate actions (same text => same action).
        dedup = []
        seen = set()
        for outs in candidates:
            t = outs[0] if outs else ""
            if t in seen:
                continue
            seen.add(t)
            dedup.append(outs)
        candidates = dedup

        self.usage.agent_runs += self.max_children

        self.usage.prompt += p_len
        for outs in candidates:
            if outs:
                self.usage.generated += _text_token_len_agent(self.mas.agents[d], outs[0])

        vals: List[float] = []
        child_trajs: List[Dict[int, List[str]]] = []
        child_texts: List[str] = []

        # Compute values for each candidate
        for outs in candidates:
            traj2 = {**node.traj, d: outs}
            child_trajs.append(traj2)
            child_texts.append(outs[0] if outs else "")

            if self.score_type == "prm":
                v = 0.0 if self.score_fn is None else float(self.score_fn(self._state_text(traj2)))

                if self.score_fn is not None:
                    self.usage.prm_calls += 1
                if self.prm_tokenizer is not None:
                    self.usage.scorer += _text_token_len_tok(
                        self.prm_tokenizer, self._state_text(traj2)
                    )

            elif self.score_type == "logprob":
                v_lp, _, g_len = compute_logprob_and_token_counts(
                    self.mas.agents[d], None, outs[0], prompt_ids=enc
                )
                self.usage.scorer += p_len + g_len
                if self.logprob_agg in ("avg_token", "avg", "mean_token"):
                    v = v_lp / max(1, g_len)
                else:  # "sum" or "avg_step" (per-step = sum here)
                    v = v_lp

            else:  # "orm" does not score partials
                v = 0.0

            vals.append(v)

        # 2) compute priors for PUCT (softmax over v_init); safe even if UCT
        priors = [1.0 / len(candidates)] * len(candidates) if candidates else []
        if self.uct_type == "puct" and candidates:
            can_logprob = getattr(self.mas.agents[d], "model", None) is not None
            if can_logprob:
                try:
                    logpriors = []
                    for outs in candidates:
                        text = outs[0] if outs else ""
                        lp, sp, sg = compute_logprob_and_token_counts(
                            self.mas.agents[d], None, text, prompt_ids=enc
                        )
                        self.usage.scorer += sp + sg
                        logpriors.append(lp / max(1, sg))

                    m = max(logpriors)
                    exps = [math.exp(lp - m) for lp in logpriors]
                    Z = sum(exps) if exps else 1.0
                    priors = [e / Z for e in exps]
                except Exception:
                    pass  # keep uniform

        # 3) create children with v_init and prior
        for v, traj2, a_text, p0 in zip(vals, child_trajs, child_texts, priors):
            node.children.append(
                _Node(
                    traj=traj2,
                    children=[],
                    visits=0,
                    q_sum=0.0,
                    v_init=v,  # keep as cached eval if you want, but not as a fake visit
                    action_text=a_text,
                    prior=p0,
                )
            )

    def rollout(self) -> None:
        node = self.root
        path = [node]

        while not self._terminal(node):
            if not node.children:
                self._expand(node)
                if not node.children:
                    break
                node = max(node.children, key=lambda ch: self._select_score(path[-1], ch))
                path.append(node)
            else:
                node = max(node.children, key=lambda ch: self._select_score(path[-1], ch))
                path.append(node)

        leaf_traj = path[-1].traj
        leaf_type = self.leaf or self.score_type

        if leaf_type == "prm":
            v_leaf = path[-1].v_init  # cached score for this node

        elif leaf_type == "logprob":
            steps = [leaf_traj[i][0] for i in range(len(leaf_traj)) if leaf_traj.get(i)]
            v_leaf, u = score_path_logprob(self.mas, self.q, steps, agg=self.logprob_agg)
            self.usage.add(u)

        else:  # "orm"
            inbox, primary_out, last = _replay_trajectory(self.mas, self.q, leaf_traj)
            ans = _aggregate_final(self.mas, primary_out, last)
            v_leaf, u = score_orm_end(
                self.leaf_score_fn, ans, tokenizer=self.orm_tokenizer
            )  # use leaf scorer
            self.usage.add(u)
            self.usage.prm_calls += 1

        for n in path:
            n.visits += 1
            n.q_sum += v_leaf

    def decode(self, n_simulations: int = 40) -> Tuple[str, TokenStats]:
        for _ in range(n_simulations):
            self.rollout()
        traj = {}
        node = self.root
        while not self._terminal(node) and node.children:
            node = max(node.children, key=lambda ch: ch.q_mean)
            traj = node.traj

        inbox, primary_out, last = _replay_trajectory(self.mas, self.q, traj)
        return _aggregate_final(self.mas, primary_out, last), self.usage

    def get_topk_answers(self, k: int = 5) -> List[str]:
        """
        Collect up to k final answers from the current MCTS tree, ranked by node q_mean.
        This only replays stored trajectories (no extra model calls).
        """
        if k <= 0:
            return []

        candidates: List[Tuple[float, str]] = []
        stack = [self.root]
        while stack:
            node = stack.pop()
            # Use nodes that can serve as final states (leaf or terminal) and that have a trajectory.
            if node.traj and self._terminal(node) and node.visits > 0:
                inbox, primary_out, last = _replay_trajectory(self.mas, self.q, node.traj)
                ans = _aggregate_final(self.mas, primary_out, last)
                candidates.append((node.q_mean, ans))
            stack.extend(node.children)

        if not candidates:
            return []

        candidates.sort(key=lambda x: x[0], reverse=True)
        return [ans for _, ans in candidates[:k]]


# =========================================================
#                  Ray worker + evaluator
# =========================================================


@dataclass
class WorkerInit:
    # MAS graph
    agent_specs: List[Dict[str, Any]]
    edges: List[List[int]]
    step_separator: str = "</step>"

    # Models
    gen_model_id: str = "Qwen/Qwen2.5-1.5B-Instruct"
    prm_dir: Optional[str] = None
    prm_base_model_id: Optional[str] = None
    orm_dir: Optional[str] = None

    # OpenAI config
    use_openai: bool = False
    openai_api_key: Optional[str] = None
    openai_base_url: Optional[str] = None

    # dtype options: "float16" | "bfloat16" | "float32"
    dtype: str = "float16"
    seed: int = 0


@dataclass
class ConditionSpec:
    name: str
    kind: str  # 'sbs_none' | 'sbs_prm' | 'sbs_logprob' | 'mcts_prm' | 'mcts_logprob' | 'mcts_orm' | 'voter_plain' | 'voter_prm' | 'voter_logprob' | 'voter_orm'
    params: Dict[str, Any] = field(default_factory=dict)


def _dtype_from_str(s: str) -> torch.dtype:
    s = (s or "").lower()
    if s in ("float16", "fp16", "half"):
        return torch.float16
    if s in ("bfloat16", "bf16"):
        return torch.bfloat16
    if s in ("float32", "fp32", "full", "float"):
        return torch.float32
    return torch.float16


def evaluate_conditions_ray(
    data: List[Dict[str, str]],
    worker_init: WorkerInit,
    conditions: List[ConditionSpec],
    name_dataset: str,
) -> Dict[str, Dict[str, float]]:
    """
    Create a Ray worker pool ONCE (models initialized in each worker),
    then evaluate multiple conditions in series while reusing that pool.

    Returns: {name: {"accuracy": float, "tok_mean": float, "tok_std": float}}
    """
    # We will compute pass@k for these k values (k is over *independent runs* of the method)
    MAX_PASS_K = 10

    # --- Ray init (idempotent)
    if not ray.is_initialized():
        # Best-effort working_dir/PYTHONPATH for cluster workers
        src_dir = str(Path(__file__).resolve().parent.parent)  # .../src
        existing = os.environ.get("PYTHONPATH", "")

        env_vars_to_pass = {
            "PYTHONPATH": f"{existing}:{src_dir}" if existing else src_dir,
            # Pass the cache location to the workers
            "HF_HOME": os.environ.get("HF_HOME", "/tmp/hf"),
            "HF_HUB_CACHE": os.environ.get("HF_HUB_CACHE", ""),
            # Pass the offline mode setting
            "HF_HUB_OFFLINE": os.environ.get("HF_HUB_OFFLINE", "0"),
        }

        runtime_env = {
            "env_vars": env_vars_to_pass,
            "working_dir": str(Path(__file__).resolve().parents[2]),  # repo root
        }
        ray.init(ignore_reinit_error=True, log_to_driver=True, runtime_env=runtime_env)

    cluster_gpus = int(ray.cluster_resources().get("GPU", 0))
    workers_per_gpu = max(1, int(os.environ.get("WORKERS_PER_GPU", "1")))
    override = os.environ.get("RAY_NUM_WORKERS", None)

    if override:
        num_workers = int(override)
        per_worker_gpu = (1.0 * cluster_gpus / num_workers) if cluster_gpus > 0 else 0.0
    else:
        if cluster_gpus > 0:
            num_workers = max(1, cluster_gpus * workers_per_gpu)
            per_worker_gpu = 1.0 / workers_per_gpu
        else:
            num_workers = os.cpu_count() or 1
            per_worker_gpu = 0.0

    # --- Worker actor --
    @ray.remote(num_cpus=1, num_gpus=per_worker_gpu)
    class EvalWorker:
        def __init__(self, init: WorkerInit, dtype: str, rank: int):
            self.rank = int(rank)
            worker_seed = int((init.seed or 0) + 1009 * self.rank + 17)
            _seed_everything(worker_seed)
            self.step_sep = init.step_separator
            self.agent_specs = init.agent_specs
            self.edges = init.edges
            self.dtype = _dtype_from_str(dtype)

            # 1) Load PRM scorer (optional)
            self.prm_score_fn = None
            self.prm_tok = None
            self.prm_model = None
            if init.prm_dir:
                self.prm_score_fn, self.prm_tok, self.prm_model = load_prm_scorer(
                    init.prm_dir,
                    base_model_id=init.prm_base_model_id,
                    torch_dtype=self.dtype,
                )

                # modify the PRM tokenizer/model with the separator
                added = ensure_separator_token(self.prm_tok, self.step_sep)
                print(f"[Worker {self.rank}] Added {added} special tokens for step separator.")
                if added > 0 and self.prm_model is not None:
                    align_model_to_tokenizer(self.prm_model, self.prm_tok)
            # 2) Load generation model; reuse PRM tokenizer if present to keep vocab identical
            self.use_openai = init.use_openai
            self.openai_client = None
            self.openai_model_name = (
                init.gen_model_id
            )  # Use gen_model_id as the OpenAI model name (e.g. gpt-4)

            if self.use_openai:
                self.policy_model = None
                self.policy_tok = AutoTokenizer.from_pretrained(
                    init.gen_model_id if "/" in init.gen_model_id else "Qwen/Qwen2.5-1.5B-Instruct",
                    use_fast=True,
                    trust_remote_code=True,
                )
                self.openai_client = OpenAI(
                    api_key=init.openai_api_key, base_url=init.openai_base_url
                )

            else:
                # Original logic
                self.policy_model, self.policy_tok = load_generation_model(
                    init.gen_model_id,
                    tokenizer=None,
                    torch_dtype=self.dtype,
                )
            # # 3) Ensure step token present in tokenizer & resize both heads if added
            # added = ensure_separator_token(self.prm_tok, self.step_sep)
            # print(f"[Worker {self.rank}] Added {added} special tokens for step separator.")
            # # Only align if we actually have a local model
            # if added > 0 and self.policy_model is not None:
            #     align_model_to_tokenizer(self.policy_model, self.policy_tok)
            # if added > 0 and self.prm_model is not None:
            #     align_model_to_tokenizer(self.prm_model, self.prm_tok)

            # 4) ORM scorer (optional) â€” same API as PRM loader
            self.orm_score_fn = None
            self.orm_tok = None
            if init.orm_dir:
                self.orm_score_fn, self.orm_tok, _ = load_prm_scorer(
                    init.orm_dir,
                    base_model_id=None,  # read from adapter config
                    torch_dtype=self.dtype,
                )

        # --- build MAS graph for each decode
        def _build_mas(self) -> MAS:
            agents = []
            for spec in self.agent_specs:
                agents.append(
                    Agent(
                        self.policy_model,
                        self.policy_tok,
                        system_prompt=spec.get("system_prompt", ""),
                        max_new_tokens=int(spec.get("max_new_tokens", 512)),
                        # OpenAI args
                        use_openai=self.use_openai,
                        openai_client=self.openai_client,
                        openai_model=self.openai_model_name,
                    )
                )
            return MAS(self.edges, agents)

        # --- decoders by spec.kind
        def _decode_by_spec(
            self, q: str, spec: ConditionSpec
        ) -> tuple[str, List[str], int, int, int]:
            kind = spec.kind
            p = spec.params or {}

            # Defaults
            sbs_kwargs = p.get(
                "gen_kwargs", {"temperature": 1.0, "top_p": 0.95, "max_new_tokens": 1024}
            )
            mcts_kwargs = p.get(
                "mcts_kwargs", {"temperature": 0.6, "top_p": 0.95, "max_new_tokens": 1024}
            )
            B1 = int(p.get("B1", 1))
            B2 = int(p.get("B2", 5))
            n_sims = int(p.get("n_simulations", 40))
            max_children = int(p.get("max_children", 3))
            c_uct = float(p.get("c_uct", 2.0))
            k = int(p.get("k", 5))

            pass_k = max(1, int(p.get("pass_k", 5)))

            # Helper: single-run decode with trace (policy-only)
            def base_policy_with_trace(mas: MAS, q: str):
                pred, usage, tr = sbs_decode2(
                    mas,
                    q,
                    score_type="none",
                    B1=1,
                    B2=1,
                    gen_kwargs=sbs_kwargs,
                    step_separator=self.step_sep,
                    return_trace=True,
                )
                return pred, usage, tr

            # Build MAS for the first run
            mas0 = self._build_mas()

            #  SBS variants
            if kind == "sbs_none":
                finals: List[str] = []
                pred, usage = sbs_decode2(
                    mas0,
                    q,
                    score_type="none",
                    B1=B1,
                    B2=B2,
                    gen_kwargs=sbs_kwargs,
                    step_separator=self.step_sep,
                    final_answers_out=finals,
                )
                candidates = finals[:pass_k] if finals else [pred]
                return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

            if kind == "sbs_prm":
                finals: List[str] = []
                pred, usage = sbs_decode2(
                    mas0,
                    q,
                    score_type="prm",
                    score_fn=self.prm_score_fn,
                    prm_tokenizer=self.prm_tok,
                    B1=B1,
                    B2=B2,
                    gen_kwargs=sbs_kwargs,
                    step_separator=self.step_sep,
                    final_answers_out=finals,
                )
                candidates = finals[:pass_k] if finals else [pred]
                return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

            if kind == "sbs_logprob":
                finals: List[str] = []
                pred, usage = sbs_decode2(
                    mas0,
                    q,
                    score_type="logprob",
                    B1=B1,
                    B2=B2,
                    gen_kwargs=sbs_kwargs,
                    step_separator=self.step_sep,
                    logprob_agg=p.get("logprob_agg", "sum"),
                    final_answers_out=finals,
                )
                candidates = finals[:pass_k] if finals else [pred]
                return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

            #  MCTS variants
            if kind in ("mcts_prm", "mcts_logprob", "mcts_orm"):
                score_type = {"mcts_prm": "prm", "mcts_logprob": "logprob", "mcts_orm": "orm"}[kind]
                score_fn = (
                    self.prm_score_fn
                    if score_type == "prm"
                    else (self.orm_score_fn if score_type == "orm" else None)
                )
                prm_tok = self.prm_tok if score_type == "prm" else None
                orm_tok = self.orm_tok if score_type == "orm" else None

                leaf_type = p.get("leaf_score_type", None)
                leaf_fn = (
                    self.orm_score_fn
                    if (leaf_type == "orm")
                    else (self.prm_score_fn if (leaf_type == "prm") else score_fn)
                )

                infer = MCTSInfer(
                    mas0,
                    q,
                    score_fn,
                    score_type=score_type,
                    prm_tokenizer=prm_tok,
                    orm_tokenizer=orm_tok,
                    max_children=max_children,
                    c_uct=c_uct,
                    gen_kwargs=mcts_kwargs,
                    step_separator=self.step_sep,
                    uct_type=p.get("uct_type", "uct"),
                    leaf_score_type=p.get("leaf_score_type", None),
                    logprob_agg=p.get("logprob_agg", "sum"),
                    leaf_score_fn=leaf_fn,
                )
                pred, usage = infer.decode(n_simulations=n_sims)
                try:
                    candidates = infer.get_topk_answers(k=pass_k)
                except Exception:
                    candidates = []
                if not candidates:
                    candidates = [pred]
                return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

            #  Majority voters
            if kind == "voter_plain":
                preds: List[str] = []
                total = TokenStats()
                p0, u0, _ = base_policy_with_trace(mas0, q)
                preds.append(p0)
                total.add(u0)
                for _ in range(max(0, k - 1)):
                    mas_i = self._build_mas()
                    pi, ui, _ = base_policy_with_trace(mas_i, q)
                    preds.append(pi)
                    total.add(ui)
                pred = _majority_by_numbers_equal(preds)
                candidates = preds[:pass_k] if pass_k > 0 else [pred]
                return pred, candidates, total.generated, total.prm_calls, total.agent_runs

            if kind in ("voter_prm", "voter_logprob", "voter_orm"):
                # Build make_scored_voter *inside* the actor (no cross-process capture)
                score_mode = {"voter_prm": "prm", "voter_logprob": "logprob", "voter_orm": "orm"}[
                    kind
                ]

                def base_with_trace(mas: MAS, q: str):
                    return sbs_decode2(
                        mas,
                        q,
                        score_type="none",
                        B1=1,
                        B2=1,
                        gen_kwargs=sbs_kwargs,
                        step_separator=self.step_sep,
                        return_trace=True,
                    )

                voter = make_scored_voter(
                    base_with_trace,
                    self._build_mas,
                    score_mode=score_mode,
                    score_fn=(
                        self.prm_score_fn
                        if score_mode == "prm"
                        else (self.orm_score_fn if score_mode == "orm" else None)
                    ),
                    prm_tokenizer=(self.prm_tok if score_mode == "prm" else None),
                    orm_tokenizer=self.orm_tok if score_mode == "orm" else None,
                    step_separator=self.step_sep,
                    k=k,
                    logprob_agg=p.get("logprob_agg", "sum"),
                    return_all_answers=True,
                )
                res = voter(mas0, q)
                if isinstance(res, tuple) and len(res) == 3:
                    pred, usage, raw_answers = res
                    candidates = (
                        list(raw_answers[:pass_k]) if isinstance(raw_answers, list) else [pred]
                    )
                elif isinstance(res, tuple) and len(res) == 2:
                    pred, usage = res
                    candidates = [pred]
                else:
                    pred = str(res)
                    usage = TokenStats()
                    candidates = [pred]
                return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

            # Fallback: do a single greedy
            finals: List[str] = []
            pred, usage = sbs_decode2(
                mas0,
                q,
                score_type="none",
                B1=1,
                B2=1,
                gen_kwargs=sbs_kwargs,
                step_separator=self.step_sep,
                final_answers_out=finals,  # NEW
            )
            candidates = finals[:pass_k] if finals else [pred]
            return pred, candidates, usage.generated, usage.prm_calls, usage.agent_runs

        # --- queue consumer
        def consume(self, spec: ConditionSpec, in_q: "Queue", out_q: "Queue"):
            try:
                while True:
                    item = in_q.get()
                    if item is None:
                        break
                    idx, q, gold, ex_seed = item
                    _seed_everything(int(ex_seed))
                    try:
                        res = self._decode_by_spec(q, spec)
                        # normalize length (be paranoid)
                        if isinstance(res, tuple):
                            if len(res) == 5:
                                pred, cand_list, tok_total, prm_calls, agent_runs = res
                            elif len(res) == 4:
                                pred, tok_total, prm_calls, agent_runs = res
                                cand_list = [pred]
                            elif len(res) == 2:
                                pred, tok_total = res
                                cand_list = [pred]
                                prm_calls = 0
                                agent_runs = 0
                            else:
                                pred = str(res[0])
                                cand_list = [pred]
                                tok_total = 0
                                prm_calls = 0
                                agent_runs = 0
                        else:
                            pred = str(res)
                            cand_list = [pred]
                            tok_total = 0
                            prm_calls = 0
                            agent_runs = 0
                    except Exception as e:
                        # surface the error as a record instead of killing the actor
                        pred = f"[ERROR] {e}"
                        cand_list = [pred]
                        tok_total = 0
                        prm_calls = 0
                        agent_runs = 0
                    out_q.put((idx, pred, cand_list, tok_total, prm_calls, agent_runs, gold))
            finally:
                # Always signal completion so the driver doesnâ€™t hang.
                out_q.put(None)

    # --- Create worker pool ONCE (reused across conditions)
    workers = [
        EvalWorker.remote(worker_init, worker_init.dtype, rank=i) for i in range(num_workers)
    ]

    # --- Evaluate all conditions, reusing the pool
    results: Dict[str, Dict[str, float]] = {}
    LOG_EVERY = int(os.environ.get("LOG_EVERY", "10"))  # progress write interval (examples)
    N_TOTAL = len(data)
    for spec in conditions:
        # Prepare conditional path parts
        prm = (
            f"_{Path(worker_init.prm_dir).name}"
            if worker_init.prm_dir and spec.kind in {"sbs_prm", "mcts_prm", "voter_prm"}
            else ""
        )
        orm = (
            f"_{Path(worker_init.orm_dir).name}"
            if worker_init.orm_dir and spec.kind in {"mcts_orm", "voter_orm"}
            else ""
        )

        # Prepare model ID and truncated params
        model = worker_init.gen_model_id.split("/")[-1]
        params = "_".join(f"{k}-{v}" for k, v in spec.params.items())[:20]

        # Assemble final path
        results_path = (
            f"logs/{name_dataset}_{spec.name}{params}{prm}{orm}_{model}_{worker_init.seed}.txt"
        )
        os.makedirs("logs", exist_ok=True)
        # If file already contains a COMPLETED marker, skip this condition
        if Path(results_path).exists():
            try:
                txt = Path(results_path).read_text()
            except Exception:
                txt = ""
            if "COMPLETED" in txt:
                print(f"[skip] Found completed log for '{spec.name}' at {results_path}. Skipping.")
                continue
        # Write a start header (append if rerunning)
        with open(results_path, "a") as f:
            f.write(f"=== START {spec.name} | dataset={name_dataset} | N={N_TOTAL} ===\n")

        in_q = Queue()
        out_q = Queue()
        consume_refs = [w.consume.remote(spec, in_q, out_q) for w in workers]

        base_seed = int(getattr(worker_init, "seed", 0) or 0)
        for idx, ex in enumerate(data):
            ex_seed = base_seed + idx
            in_q.put((idx, ex["question"], ex["answer"], ex_seed))
        for _ in range(len(workers)):
            in_q.put(None)

        correct = 0  # pass@1 on the best prediction
        pass_counts = [0] * MAX_PASS_K  # pass@k over candidate sets
        totals: List[int] = []
        finished = 0
        prm_calls_total = 0
        agent_runs_total = 0
        pbar = tqdm(total=len(data), desc=f"Evaluating: {spec.name}")
        while finished < len(workers):
            try:
                # Try to get an item with a timeout (e.g., 5 seconds)
                item = out_q.get(timeout=5)
            except queue.Empty:
                ready_refs, _ = ray.wait(consume_refs, timeout=0)

                if ready_refs:
                    # If a worker is "ready" (finished) but we didn't get a None in the queue,
                    for ref in ready_refs:
                        try:
                            ray.get(ref)  # This will raise the worker's exception if it crashed
                        except Exception as e:
                            print(f"\n[CRITICAL] A worker crashed! Error: {e}")
                            finished += 1
                            consume_refs.remove(ref)
                continue
            if item is None:
                finished += 1
                continue
            idx, pred, cand_list, tok_total, prm_calls, agent_runs, gold = item

            # Normalize candidate list and ensure pred is first
            if not isinstance(cand_list, (list, tuple)):
                cand_list = [cand_list]
            ordered_cands: List[str] = [pred] + [c for c in cand_list if c != pred]
            if not ordered_cands:
                ordered_cands = [pred]
            ordered_cands = ordered_cands[:MAX_PASS_K]

            # pass@1 (accuracy on top-1)
            if is_correct(pred, gold):
                correct += 1

            # pass@k: any correct among the first k candidates
            any_correct = False
            for ki in range(MAX_PASS_K):
                if ki < len(ordered_cands) and is_correct(ordered_cands[ki], gold):
                    any_correct = True
                if any_correct:
                    pass_counts[ki] += 1

            if tok_total is not None:
                totals.append(int(tok_total))
            prm_calls_total += int(prm_calls or 0)
            agent_runs_total += int(agent_runs or 0)
            pbar.update(1)

            # periodic progress logging
            seen = len(totals)
            if seen % LOG_EVERY == 0:
                acc_so_far = correct / max(1, seen)
                mean_so_far = float(statistics.mean(totals)) if totals else 0.0
                std_so_far = float(statistics.pstdev(totals)) if len(totals) > 1 else 0.0
                pass_str = ", ".join(
                    f"p@{i+1}={pass_counts[i]/max(1, seen):.4f}" for i in range(MAX_PASS_K)
                )
                with open(results_path, "a") as f:
                    f.write(
                        "[progress] "
                        f"{seen}/{N_TOTAL} "
                        f"acc={acc_so_far:.4f} "
                        f"{pass_str} "
                        f"tokens={mean_so_far:.1f}Â±{std_so_far:.1f} "
                        f"prm_calls_total={prm_calls_total} "
                        f"agent_runs_total={agent_runs_total} "
                        f"prm_calls_mean={prm_calls_total/max(1, seen):.2f} "
                        f"agent_runs_mean={agent_runs_total/max(1, seen):.2f}\n"
                    )

        pbar.close()

        # Ensure tasks done (actors persist for next spec)
        ray.get(consume_refs)

        n = max(1, len(data))
        acc = correct / n
        pass_at = [c / n for c in pass_counts]
        mean = float(statistics.mean(totals)) if totals else 0.0
        std = float(statistics.pstdev(totals)) if len(totals) > 1 else 0.0

        results[spec.name] = {
            "accuracy": acc,
            "tok_mean": mean,
            "tok_std": std,
            "prm_calls_total": int(prm_calls_total),
            "agent_runs_total": int(agent_runs_total),
            "prm_calls_mean": float(prm_calls_total) / n,
            "agent_runs_mean": float(agent_runs_total) / n,
        }
        for i, v in enumerate(pass_at, start=1):
            results[spec.name][f"pass_at_{i}"] = v

        # Append final summary and mark as completed
        with open(results_path, "a") as f:
            f.write(f"Accuracy (pass@1): {acc:.4f}\n")
            for i in range(2, MAX_PASS_K + 1):
                f.write(f"Pass@{i}: {pass_at[i-1]:.4f}\n")
            f.write(f"Tokens: {mean:.1f} Â± {std:.1f} ({len(totals)} examples)\n")
            f.write(f"Total PRM calls: {prm_calls_total}\n")
            f.write(f"Total Agent runs: {agent_runs_total}\n")
            f.write(f"Mean PRM calls per example: {results[spec.name]['prm_calls_mean']:.2f}\n")
            f.write(f"Mean Agent runs per example: {results[spec.name]['agent_runs_mean']:.2f}\n")
            f.write("COMPLETED\n")

        pass_str = " ".join(f"p@{i+1}={pass_at[i]:.4f}" for i in range(MAX_PASS_K))
        print(
            f"[result] {spec.name}: acc={acc:.4f} ({pass_str})  "
            f"tokens={mean:.1f}Â±{std:.1f} ({len(totals)} examples)"
        )

    # Optionally keep workers alive (faster for follow-ups). If you want to free VRAM at end:
    # for w in workers: ray.kill(w, no_restart=True)

    return results
